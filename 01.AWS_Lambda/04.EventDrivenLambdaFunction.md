
### DynamoDB Stream Configuration Details

1. **Activate trigger: Yes**
   - Indicates that the trigger between the DynamoDB stream and the Lambda function is active. This means that the Lambda function is currently set to be invoked by the stream.

2. **Batch size: 100**
   - The maximum number of records that Lambda will retrieve from the DynamoDB stream in a single batch. This is the number of records processed in one invocation of the Lambda function.

3. **Batch window: 57**
   - The maximum amount of time, in seconds, that Lambda waits before processing a batch of records. Lambda processes the batch when the batch size is reached or when the batch window expires, whichever comes first.

4. **Concurrent batches per shard: 1**
   - The number of batches that Lambda processes concurrently per shard. Setting this to 1 means that Lambda processes one batch at a time for each shard.

5. **Last processing result: OK**
   - Indicates the result of the last batch processing. "OK" means the last batch was processed successfully.

6. **Maximum age of record: -1**
   - The maximum age of a record that Lambda will process. A value of -1 means there is no limit on the age of the records.

7. **On-failure destination: None**
   - Indicates there is no destination configured to receive records that failed to be processed by Lambda.

8. **Report batch item failures: No**
   - Indicates whether Lambda reports individual record processing failures within a batch. "No" means it does not report individual record failures.

9. **Retry attempts: -1**
   - The number of retry attempts for processing a batch of records. A value of -1 means there is no limit on the number of retries.

10. **Split batch on error: No**
    - Indicates whether Lambda splits a batch into smaller batches and retries if an error occurs. "No" means the entire batch will be retried on error.

11. **Starting position: LATEST**
    - The position in the stream where Lambda starts reading records. "LATEST" means it starts from the most recent record.

12. **Tumbling window duration: None**
    - Indicates there is no tumbling window configured. Tumbling windows are used for grouping records into fixed-size, non-overlapping windows.

13. **UUID: d71dc3f9-c83d-4019-9852-1dc9cfe67a6b**
    - A unique identifier for the event source mapping.


### Reducing Lambda Costs

To reduce the cost associated with Lambda invocations from a DynamoDB stream, consider the following optimizations:

1. **Optimize Batch Size and Batch Window**:
   - **Batch size**: Adjust the batch size to ensure that Lambda processes the maximum number of records in a single invocation. If your Lambda function can handle it, increasing the batch size can reduce the number of invocations and hence the cost.
   - **Batch window**: Increase the batch window to accumulate more records before processing. This can help ensure that batches are fully utilized, reducing the number of invocations.

2. **Concurrent Batches per Shard**:
   - If your processing logic allows for it, you could increase the concurrent batches per shard to process multiple batches in parallel, which can increase throughput and reduce time spent processing records. However, this can also increase the cost if not managed properly.

3. **Retry Attempts and Split Batch on Error**:
   - Set a limit on retry attempts (e.g., 2 or 3 retries) to avoid indefinite retries that can rack up costs. 
   - Enable "Split batch on error" if you often encounter individual record failures, which can help process successful records without retrying the entire batch.

4. **Use On-Failure Destination**:
   - Configure an on-failure destination (e.g., an SQS queue or SNS topic) to handle failed records. This can prevent repeated processing failures from increasing costs.

5. **Process Only Necessary Records**:
   - Use filters or conditional logic in your Lambda function to ensure that only necessary records are processed. This can reduce the number of invocations and the amount of data processed.

6. **Monitor and Adjust**:
   - Regularly monitor your Lambda metrics and DynamoDB stream activity in CloudWatch. Use this data to fine-tune your configurations to ensure efficient processing and cost management.

By carefully tuning these parameters, you can optimize the cost-effectiveness of your Lambda functions triggered by DynamoDB streams.

# Event
```
{
  'Records': [
    {
      'eventID': '6c9dc99d111d79836eebb3a1b1989013',
      'eventName': 'MODIFY',
      'eventVersion': '1.1',
      'eventSource': 'aws:dynamodb',
      'awsRegion': 'us-east-1',
      'dynamodb': {
        'ApproximateCreationDateTime': 1720629863.0,
        'Keys': {
          'id': {
            'S': '2'
          }
        },
        'NewImage': {
          'fname': {
            'S': 'Gaurav'
          },
          'number': {
            'S': '334433223'
          },
          'lname': {
            'S': 'Kandel'
          },
          'id': {
            'S': '2'
          }
        },
        'OldImage': {
          'fname': {
            'S': 'Saurav'
          },
          'number': {
            'S': '334433223'
          },
          'lname': {
            'S': 'Kandel'
          },
          'id': {
            'S': '2'
          }
        },
        'SequenceNumber': '5100000000093649950825',
        'SizeBytes': 83,
        'StreamViewType': 'NEW_AND_OLD_IMAGES'
      },
      'eventSourceARN': 'arn:aws:dynamodb:us-east-1:381491882333:table/student/stream/2024-07-10T15:55:45.474'
    }
  ]
}
```


# Lambda Code
```python
from datetime import datetime
import pandas as pd
import boto3
from io import StringIO

def handle_insert(record):
    print("Handling Insert: ", record)
    dict = {}

    for key, value in record['dynamodb']['NewImage'].items():
        for dt, col in value.items():
            dict.update({key: col})

    dff = pd.DataFrame([dict])
    dff['EventType'] = record['eventName']
    return dff

def handle_modify(record):
    print("Handling Modify: ", record)
    dict = {}

    for key, value in record['dynamodb']['NewImage'].items():
        for dt, col in value.items():
            dict.update({key: col})

    dff_insert = pd.DataFrame([dict])
    dff_insert['EventType'] = "INSERT"

    dict = {}

    for key, value in record['dynamodb']['OldImage'].items():
        for dt, col in value.items():
            dict.update({key: col})

    dff_remove = pd.DataFrame([dict])
    dff_remove['EventType'] = "REMOVE"

    return pd.concat([dff_insert, dff_remove], ignore_index=True)

def handle_remove(record):
    print("Handle Remove: ", record)
    dict = {}

    for key, value in record['dynamodb']['OldImage'].items():
        for dt, col in value.items():
            dict.update({key: col})

    dff = pd.DataFrame([dict])
    dff['EventId'] = record['eventID']
    dff['EventType'] = record['eventName']
    return dff

def lambda_handler(event, context):
    print(event)
    df = pd.DataFrame()

    for record in event['Records']:
        table = record['eventSourceARN'].split("/")[1]

        if record['eventName'] == "INSERT": 
            dff = handle_insert(record)
        elif record['eventName'] == "MODIFY":
            dff = handle_modify(record)
        elif record['eventName'] == "REMOVE":
            dff = handle_remove(record)
        else:
            continue

        if dff is not None:
            dff['created_at'] = record['dynamodb']['ApproximateCreationDateTime']
        df = dff

    if not df.empty:
        all_columns = list(df)
        df[all_columns] = df[all_columns].astype(str)

        path = table + "_" + str(datetime.now()) + ".csv"
        print(event)

        csv_buffer = StringIO()
        df.to_csv(csv_buffer,index=False)

        s3 = boto3.client('s3')
        bucketName = "project-de-datewithdata"
        key = "staging/" + table + "/" + table + "_" + str(datetime.now()) + ".csv"
        print(key)
        
        s3.put_object(Bucket=bucketName, Key=key, Body=csv_buffer.getvalue(),)

    print('Successfully processed %s records.' % str(len(event['Records'])))
```